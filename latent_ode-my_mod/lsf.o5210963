Lmod has detected the following error: The following module(s) are unknown:
"pytorch/1.4.0"

Please check the spelling or version number. Also try "module spider ..."
It is also possible your cache file is out-of-date; it may help to try:
  $ module --ignore-cache load "pytorch/1.4.0"



/cluster/work/igp_psr/metzgern/ODEcrop/latent_ode-my_mod/run_models.py
run_models.py --niters 7 -n 300000 -b 2000 -l 35 --dataset crop --ode-rnn --rec-dims 100 --rec-layers 4 --gen-layers 1 --units 500 --gru-units 50 --classif --ode-method euler
  0%|          | 0/1008 [00:00<?, ?it/s]  0%|          | 1/1008 [02:03<34:32:49, 123.50s/it]  0%|          | 2/1008 [04:06<34:27:37, 123.32s/it]  0%|          | 3/1008 [06:09<34:24:35, 123.26s/it]  0%|          | 4/1008 [08:12<34:23:12, 123.30s/it]  0%|          | 5/1008 [10:16<34:23:50, 123.46s/it]  1%|          | 6/1008 [12:20<34:20:51, 123.40s/it]Experiment 92276
Epoch 0000 [Test seq (cond on sampled tp)] | Loss 2.041007 | Likelihood -1026.828613 | KL fp 0.0000 | FP STD 0.0000|
KL coef: 0.0
Train loss (one batch): 2.0389668941497803
Train CE loss (one batch): 2.0389668941497803
Classification accuracy (TRAIN): 0.3320
Test MSE: 0.2061
Classification accuracy (TEST): 0.3522
Poisson likelihood: 0.0
CE loss: 2.0410068035125732
-----------------------------------------------------------------------------------
  1%|          | 7/1008 [15:43<40:58:12, 147.34s/it]  1%|          | 8/1008 [17:37<38:10:40, 137.44s/it]  1%|          | 9/1008 [19:23<35:33:27, 128.14s/it]  1%|          | 10/1008 [21:10<33:42:15, 121.58s/it]  1%|          | 11/1008 [22:55<32:19:23, 116.71s/it]  1%|          | 12/1008 [24:41<31:24:41, 113.54s/it]  1%|▏         | 13/1008 [26:26<30:41:38, 111.05s/it]Experiment 92276
Epoch 0000 [Test seq (cond on sampled tp)] | Loss 1.919435 | Likelihood -1084.123413 | KL fp 0.0000 | FP STD 0.0000|
KL coef: 0.0
Train loss (one batch): 1.9582860469818115
Train CE loss (one batch): 1.9582860469818115
Classification accuracy (TRAIN): 0.3420
Test MSE: 0.2176
Classification accuracy (TEST): 0.3536
Poisson likelihood: 0.0
CE loss: 1.919434905052185
-----------------------------------------------------------------------------------
  1%|▏         | 14/1008 [29:21<35:56:28, 130.17s/it]  1%|▏         | 15/1008 [31:07<33:52:53, 122.83s/it]  2%|▏         | 16/1008 [32:53<32:25:50, 117.69s/it]  2%|▏         | 17/1008 [34:38<31:23:31, 114.04s/it]  2%|▏         | 18/1008 [36:24<30:40:32, 111.55s/it]  2%|▏         | 19/1008 [38:09<30:06:01, 109.57s/it]  2%|▏         | 20/1008 [39:54<29:43:49, 108.33s/it]Experiment 92276
Epoch 0000 [Test seq (cond on sampled tp)] | Loss 1.809808 | Likelihood -1326.656250 | KL fp 0.0000 | FP STD 0.0000|
KL coef: 0.0
Train loss (one batch): 1.834220051765442
Train CE loss (one batch): 1.834220051765442
Classification accuracy (TRAIN): 0.3600
Test MSE: 0.2661
Classification accuracy (TEST): 0.3649
Poisson likelihood: 0.0
CE loss: 1.8098076581954956
-----------------------------------------------------------------------------------
  2%|▏         | 21/1008 [42:49<35:09:22, 128.23s/it]  2%|▏         | 22/1008 [44:34<33:15:04, 121.40s/it]  2%|▏         | 23/1008 [46:20<31:56:05, 116.72s/it]  2%|▏         | 24/1008 [48:06<30:58:32, 113.33s/it]  2%|▏         | 25/1008 [49:52<30:20:50, 111.14s/it]  3%|▎         | 26/1008 [51:37<29:48:22, 109.27s/it]  3%|▎         | 27/1008 [53:23<29:31:40, 108.36s/it]Experiment 92276
Epoch 0000 [Test seq (cond on sampled tp)] | Loss 1.586189 | Likelihood -1389.563477 | KL fp 0.0000 | FP STD 0.0000|
KL coef: 0.0
Train loss (one batch): 1.7145777940750122
Train CE loss (one batch): 1.7145777940750122
Classification accuracy (TRAIN): 0.4300
Test MSE: 0.2786
Classification accuracy (TEST): 0.5177
Poisson likelihood: 0.0
CE loss: 1.5861890316009521
-----------------------------------------------------------------------------------
  3%|▎         | 28/1008 [56:17<34:54:05, 128.21s/it]  3%|▎         | 29/1008 [58:03<33:00:07, 121.36s/it]  3%|▎         | 30/1008 [59:49<31:43:35, 116.78s/it]  3%|▎         | 31/1008 [1:01:34<30:45:20, 113.33s/it]  3%|▎         | 32/1008 [1:03:20<30:06:54, 111.08s/it]  3%|▎         | 33/1008 [1:05:05<29:36:56, 109.35s/it]  3%|▎         | 34/1008 [1:06:50<29:14:54, 108.11s/it]Experiment 92276
Epoch 0000 [Test seq (cond on sampled tp)] | Loss 1.424594 | Likelihood -1353.312622 | KL fp 0.0000 | FP STD 0.0000|
KL coef: 0.0
Train loss (one batch): 1.60933518409729
Train CE loss (one batch): 1.60933518409729
Classification accuracy (TRAIN): 0.4840
Test MSE: 0.2714
Classification accuracy (TEST): 0.5424
Poisson likelihood: 0.0
CE loss: 1.4245936870574951
-----------------------------------------------------------------------------------
  3%|▎         | 35/1008 [1:09:45<34:36:40, 128.06s/it]  4%|▎         | 36/1008 [1:11:31<32:47:07, 121.43s/it]  4%|▎         | 37/1008 [1:13:16<31:25:44, 116.52s/it]  4%|▍         | 38/1008 [1:15:02<30:34:14, 113.46s/it]  4%|▍         | 39/1008 [1:16:47<29:50:35, 110.87s/it]  4%|▍         | 40/1008 [1:18:33<29:25:53, 109.46s/it]  4%|▍         | 41/1008 [1:20:19<29:03:33, 108.18s/it]Experiment 92276
Epoch 0000 [Test seq (cond on sampled tp)] | Loss 1.339056 | Likelihood -1320.537231 | KL fp 0.0000 | FP STD 0.0000|
KL coef: 0.0
Train loss (one batch): 1.4852501153945923
Train CE loss (one batch): 1.4852501153945923
Classification accuracy (TRAIN): 0.5490
Test MSE: 0.2648
Classification accuracy (TEST): 0.5504
Poisson likelihood: 0.0
CE loss: 1.3390562534332275
-----------------------------------------------------------------------------------
  4%|▍         | 42/1008 [1:23:14<34:25:09, 128.27s/it]  4%|▍         | 43/1008 [1:25:00<32:37:40, 121.72s/it]  4%|▍         | 44/1008 [1:26:45<31:14:41, 116.68s/it]  4%|▍         | 45/1008 [1:28:31<30:22:26, 113.55s/it]  5%|▍         | 46/1008 [1:30:16<29:39:53, 111.01s/it]  5%|▍         | 47/1008 [1:32:02<29:10:46, 109.31s/it]  5%|▍         | 48/1008 [1:33:48<28:51:49, 108.24s/it]Experiment 92276
Epoch 0000 [Test seq (cond on sampled tp)] | Loss 1.160010 | Likelihood -1213.341064 | KL fp 0.0000 | FP STD 0.0000|
KL coef: 0.0
Train loss (one batch): 1.4140796661376953
Train CE loss (one batch): 1.4140796661376953
Classification accuracy (TRAIN): 0.5720
Test MSE: 0.2434
Classification accuracy (TEST): 0.6491
Poisson likelihood: 0.0
CE loss: 1.1600103378295898
-----------------------------------------------------------------------------------
  5%|▍         | 49/1008 [1:36:43<34:11:46, 128.37s/it]  5%|▍         | 50/1008 [1:38:29<32:21:16, 121.58s/it]  5%|▌         | 51/1008 [1:40:15<31:07:30, 117.09s/it]  5%|▌         | 52/1008 [1:42:01<30:10:11, 113.61s/it]  5%|▌         | 53/1008 [1:43:47<29:34:10, 111.47s/it]  5%|▌         | 54/1008 [1:45:32<29:02:36, 109.60s/it]  5%|▌         | 55/1008 [1:47:18<28:43:32, 108.51s/it]Experiment 92276
Epoch 0000 [Test seq (cond on sampled tp)] | Loss 1.081307 | Likelihood -1289.928833 | KL fp 0.0000 | FP STD 0.0000|
KL coef: 0.0
Train loss (one batch): 1.2870614528656006
Train CE loss (one batch): 1.2870614528656006
Classification accuracy (TRAIN): 0.6155
Test MSE: 0.2587
Classification accuracy (TEST): 0.6898
Poisson likelihood: 0.0
CE loss: 1.0813071727752686
-----------------------------------------------------------------------------------
  6%|▌         | 56/1008 [1:50:13<33:58:42, 128.49s/it]  6%|▌         | 57/1008 [1:51:59<32:08:53, 121.70s/it]Couldn't import umap
Sampling dataset of 300000 training examples
Dataset Crops
    Number of datapoints: 287858
    Root Location: data/Crops
    Reduce: average

Computing loss... 
Number of labeled examples: 1499
plotting....
Computing loss... 
Number of labeled examples: 1499
plotting....
Computing loss... 
Number of labeled examples: 1499
plotting....
Computing loss... 
Number of labeled examples: 1499
plotting....
Computing loss... 
Number of labeled examples: 1499
plotting....
Computing loss... 
Number of labeled examples: 1499
plotting....
Computing loss... 
Number of labeled examples: 1499
plotting....
Computing loss... 
Number of labeled examples: 1499
plotting....
Traceback (most recent call last):
  File "run_models.py", line 290, in <module>
    train_res = model.compute_all_losses(batch_dict, n_traj_samples = 3, kl_coef = kl_coef)
  File "/cluster/work/igp_psr/metzgern/ODEcrop/latent_ode-my_mod/lib/base_models.py", line 135, in compute_all_losses
    mask = (batch_dict["mask_predicted_data"]) ) # this mask gives UserWarnings # torch.BoolTensor
  File "/cluster/work/igp_psr/metzgern/ODEcrop/latent_ode-my_mod/lib/base_models.py", line 115, in get_mse
    log_density_data = compute_mse(pred_y, truth, mask = mask)
  File "/cluster/work/igp_psr/metzgern/ODEcrop/latent_ode-my_mod/lib/likelihood_eval.py", line 276, in compute_mse
    res = compute_masked_likelihood(mu, data, mask, mse)
  File "/cluster/work/igp_psr/metzgern/ODEcrop/latent_ode-my_mod/lib/likelihood_eval.py", line 192, in compute_masked_likelihood
    data_masked = torch.masked_select(data[i,k,:,j], mask[i,k,:,j].bool()) #byte()
KeyboardInterrupt
  6%|▌         | 57/1008 [1:53:18<31:30:25, 119.27s/it]
